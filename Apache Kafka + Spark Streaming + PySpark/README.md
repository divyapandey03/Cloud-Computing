# Introduction:

Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.

Kafka provides three main functions to its users:

- Publish and subscribe to streams of records
- Effectively store streams of records in the order in which records were generated
- Process streams of records in real time


Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams. It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data.  

We use Apache Kafka in a Linux environment in GCP Platform and reveals how events are consumed and produced using a Kafka-Python.The following diagram illustrates the Kafka ecosystem we’re going to set up.

<img width="553" alt="kafkaimage" src="https://user-images.githubusercontent.com/23255126/205745054-63ed168b-f050-48b4-9cbf-3089f193787e.png">

## Spark Streaming:

Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.


<img width="529" alt="d1" src="https://user-images.githubusercontent.com/23255126/205745887-a26b48ce-1dbf-455c-acf6-558bd57b6be4.png">

Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.

<img width="467" alt="d2" src="https://user-images.githubusercontent.com/23255126/205746024-8ec81aff-5f56-4150-a84e-db670f810d52.png">

## Installing Spark
Step : Download spark-2.3.2 to the local machine using the following command:
$ wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz

Step : Unpack
$ tar -xvf spark-3.3.1-bin-hadoop3.tgz

<img width="646" alt="g2" src="https://user-images.githubusercontent.com/23255126/205775687-0b33ab76-ea95-4c1c-8007-4a721e545ad9.png">

create a soft link:
$ ln -s /home/dpandey/spark-3.3.1-bin-hadoop3 /home/dpandey/spark

### Add SPARK_HOME entry to bashrc

export SPARK_HOME=/home/dpandey/spark

export PATH=$SPARK_HOME/bin:$PATH

export PATH=$SPARK_HOME/sbin:$PATH


### Load bashrc file 
 source ~/.bashrc
 
### Install Java

- sudo apt-get update
- sudo apt install default-jdk
- java --version
- update-alternatives --list java
- export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/bin/java

### Verify the installation
   $ pyspark
   
   <img width="857" alt="g3" src="https://user-images.githubusercontent.com/23255126/205776226-180c98ae-86ac-45b4-ac44-22290cbef18a.png">



